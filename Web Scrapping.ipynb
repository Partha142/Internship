{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f5bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4\n",
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a514d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer.desktop-cm1j5dq\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1c6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7c7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ffd47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers from Wikipedia:\n",
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "From today's featured list\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n",
      "\n",
      "DataFrame:\n",
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        headers = []\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            headers.append(tag.text.strip())\n",
    "        return headers\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(headers):\n",
    "    df_data = {'Headers': headers}\n",
    "    df = pd.DataFrame(df_data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    \n",
    "    headers = get_wikipedia_headers(wikipedia_url)\n",
    "    \n",
    "    if headers:\n",
    "        print(\"Headers from Wikipedia:\")\n",
    "        for header in headers:\n",
    "            print(header)\n",
    "\n",
    "        df = create_dataframe(headers)\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No headers found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d85d7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4fbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://presidentofindia.nic.in/former-presidents.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5a5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch content from https://presidentofindia.nic.in/former-presidents.htm. Status code: 404\n",
      "No data found.\n"
     ]
    }
   ],
   "source": [
    "def get_former_presidents_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        presidents_data = []\n",
    "\n",
    "        for president in soup.find_all('div', class_='former_presidents'):\n",
    "            name = president.find('h3').text.strip()\n",
    "            term_of_office = president.find('p').text.strip()\n",
    "            presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "        return presidents_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(presidents_data):\n",
    "    df = pd.DataFrame(presidents_data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    presidents_url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "    \n",
    "    former_presidents_data = get_former_presidents_data(presidents_url)\n",
    "    \n",
    "    if former_presidents_data:\n",
    "        print(\"Former Presidents of India:\")\n",
    "        for president in former_presidents_data:\n",
    "            print(f\"Name: {president['Name']}, Term of Office: {president['Term of Office']}\")\n",
    "\n",
    "        df = create_dataframe(former_presidents_data)\n",
    "        print(\"\\nDataFrame:\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7837ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799ad01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.icc-cricket.com/rankings/team-rankings/mens/odi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "809d49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_odi_team_rankings():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        teams_data = []\n",
    "\n",
    "        for team in soup.select('.rankings-block__container .rankings-block__banner, .rankings-block__container .table-body')[:10]:\n",
    "            team_name = team.select_one('.u-hide-phablet').text.strip()\n",
    "            matches = int(team.select_one('.rankings-block__banner--matches, .table-body__cell.u-center-text').text.strip())\n",
    "            points = int(team.select_one('.rankings-block__banner--points, .table-body__cell.u-center-text:nth-of-type(3)').text.strip())\n",
    "            rating = int(team.select_one('.rankings-block__banner--rating, .table-body__cell.u-text-right').text.strip())\n",
    "\n",
    "            teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "        return teams_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_odi_player_rankings(category):\n",
    "    url = f\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/{category}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        players_data = []\n",
    "\n",
    "        for player in soup.select('.table-body .table-body__row')[:10]:\n",
    "            player_name = player.select_one('.table-body__cell.rankings-table__name.name').text.strip()\n",
    "            team = player.select_one('.table-body__logo-text').text.strip()\n",
    "            rating = int(player.select_one('.table-body__cell.u-text-right.rating').text.strip())\n",
    "\n",
    "            players_data.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "        return players_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Part a) Top 10 ODI teams in men’s cricket along with the records for matches, points, and rating.\n",
    "    teams_data = scrape_odi_team_rankings()\n",
    "    if teams_data:\n",
    "        team_columns = ['Team', 'Matches', 'Points', 'Rating']\n",
    "        team_df = create_dataframe(teams_data, team_columns)\n",
    "        print(\"\\nTop 10 ODI Teams:\")\n",
    "        print(team_df)\n",
    "\n",
    "    # Part b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "    batsmen_data = scrape_odi_player_rankings('batting')\n",
    "    if batsmen_data:\n",
    "        batsmen_columns = ['Player', 'Team', 'Rating']\n",
    "        batsmen_df = create_dataframe(batsmen_data, batsmen_columns)\n",
    "        print(\"\\nTop 10 ODI Batsmen:\")\n",
    "        print(batsmen_df)\n",
    "\n",
    "    # Part c) Top 10 ODI Bowlers along with the records of their team and rating.\n",
    "    bowlers_data = scrape_odi_player_rankings('bowling')\n",
    "    if bowlers_data:\n",
    "        bowlers_columns = ['Player', 'Team', 'Rating']\n",
    "        bowlers_df = create_dataframe(bowlers_data, bowlers_columns)\n",
    "        print(\"\\nTop 10 ODI Bowlers:\")\n",
    "        print(bowlers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "549ee0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_womens_odi_team_rankings():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        teams_data = []\n",
    "\n",
    "        for team in soup.select('.rankings-block__container .rankings-block__banner, .rankings-block__container .table-body')[:10]:\n",
    "            team_name = team.select_one('.u-hide-phablet').text.strip()\n",
    "            matches = int(team.select_one('.rankings-block__banner--matches, .table-body__cell.u-center-text').text.strip())\n",
    "            points = int(team.select_one('.rankings-block__banner--points, .table-body__cell.u-center-text:nth-of-type(3)').text.strip())\n",
    "            rating = int(team.select_one('.rankings-block__banner--rating, .table-body__cell.u-text-right').text.strip())\n",
    "\n",
    "            teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "        return teams_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def scrape_womens_odi_player_rankings(category):\n",
    "    url = f\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/{category}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        players_data = []\n",
    "\n",
    "        for player in soup.select('.table-body .table-body__row')[:10]:\n",
    "            player_name = player.select_one('.table-body__cell.rankings-table__name.name').text.strip()\n",
    "            team = player.select_one('.table-body__logo-text').text.strip()\n",
    "            rating = int(player.select_one('.table-body__cell.u-text-right.rating').text.strip())\n",
    "\n",
    "            players_data.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "        return players_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Part a) Top 10 ODI teams in women’s cricket along with the records for matches, points, and rating.\n",
    "    teams_data = scrape_womens_odi_team_rankings()\n",
    "    if teams_data:\n",
    "        team_columns = ['Team', 'Matches', 'Points', 'Rating']\n",
    "        team_df = create_dataframe(teams_data, team_columns)\n",
    "        print(\"\\nTop 10 Women's ODI Teams:\")\n",
    "        print(team_df)\n",
    "\n",
    "    # Part b) Top 10 Women’s ODI Batting players along with the records of their team and rating.\n",
    "    batting_players_data = scrape_womens_odi_player_rankings('batting')\n",
    "    if batting_players_data:\n",
    "        batting_players_columns = ['Player', 'Team', 'Rating']\n",
    "        batting_players_df = create_dataframe(batting_players_data, batting_players_columns)\n",
    "        print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "        print(batting_players_df)\n",
    "\n",
    "    # Part c) Top 10 Women’s ODI All-rounders along with the records of their team and rating.\n",
    "    allrounders_data = scrape_womens_odi_player_rankings('all-rounder')\n",
    "    if allrounders_data:\n",
    "        allrounders_columns = ['Player', 'Team', 'Rating']\n",
    "        allrounders_df = create_dataframe(allrounders_data, allrounders_columns)\n",
    "        print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "        print(allrounders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e49a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2c51092",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dead5b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'url': No scheme supplied. Perhaps you meant http://url?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     25\u001b[0m     cnbc_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 27\u001b[0m     news_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_cnbc_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnbc_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m news_data:\n\u001b[0;32m     30\u001b[0m         news_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNews Link\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_cnbc_news\u001b[39m(url):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m      5\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant http://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'url': No scheme supplied. Perhaps you meant http://url?"
     ]
    }
   ],
   "source": [
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get('url')\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        news_data = []\n",
    "\n",
    "        for news_item in soup.find_all('div', class_='Card-titleLink'):\n",
    "            headline = news_item.text.strip()\n",
    "            time = news_item.find_next('time').text.strip()\n",
    "            news_link = 'https://www.cnbc.com' + news_item['href']\n",
    "\n",
    "            news_data.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "        return news_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cnbc_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    \n",
    "    news_data = scrape_cnbc_news(cnbc_url)\n",
    "    \n",
    "    if news_data:\n",
    "        news_columns = ['Headline', 'Time', 'News Link']\n",
    "        news_df = create_dataframe(news_data, news_columns)\n",
    "        print(\"\\nNews Details:\")\n",
    "        print(news_df)\n",
    "    else:\n",
    "        print(\"No news details found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830efce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5300f2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles details found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_data = []\n",
    "\n",
    "        for article in soup.find_all('li', class_='item'):\n",
    "            title = article.find('h3', class_='js-issue-item-title').text.strip()\n",
    "            authors = ', '.join([author.text.strip() for author in article.find_all('span', class_='authorName')])\n",
    "\n",
    "            published_date = article.find('div', class_='publishedOnline').text.strip()\n",
    "\n",
    "            paper_url = article.find('a', class_='pdf-link')['href']\n",
    "\n",
    "            articles_data.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "        return articles_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elsevier_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    \n",
    "    articles_data = scrape_most_downloaded_articles(elsevier_url)\n",
    "    \n",
    "    if articles_data:\n",
    "        articles_columns = ['Paper Title', 'Authors', 'Published Date', 'Paper URL']\n",
    "        articles_df = create_dataframe(articles_data, articles_columns)\n",
    "        print(\"\\nMost Downloaded Articles:\")\n",
    "        print(articles_df)\n",
    "    else:\n",
    "        print(\"No articles details found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19279e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4742e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles details found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_data = []\n",
    "\n",
    "        for article in soup.find_all('li', class_='item'):\n",
    "            title = article.find('h3', class_='js-issue-item-title').text.strip()\n",
    "            authors = ', '.join([author.text.strip() for author in article.find_all('span', class_='authorName')])\n",
    "\n",
    "            published_date = article.find('div', class_='publishedOnline').text.strip()\n",
    "\n",
    "            paper_url = article.find('a', class_='pdf-link')['href']\n",
    "\n",
    "            articles_data.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "        return articles_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def create_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elsevier_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    \n",
    "    articles_data = scrape_most_downloaded_articles(elsevier_url)\n",
    "    \n",
    "    if articles_data:\n",
    "        articles_columns = ['Paper Title', 'Authors', 'Published Date', 'Paper URL']\n",
    "        articles_df = create_dataframe(articles_data, articles_columns)\n",
    "        print(\"\\nMost Downloaded Articles:\")\n",
    "        print(articles_df)\n",
    "    else:\n",
    "        print(\"No articles details found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d5903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
