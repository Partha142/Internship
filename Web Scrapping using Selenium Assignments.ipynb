{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759e9a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "form_data = {}\n",
    "for input_tag in soup.select('form input'):\n",
    "    form_data[input_tag.get('name')] = input_tag.get('value', '')\n",
    "\n",
    "form_data['search-keyword'] = job_title\n",
    "form_data['location'] = location\n",
    "\n",
    "\n",
    "search_response = requests.post(url, data=form_data)\n",
    "\n",
    "search_soup = BeautifulSoup(search_response.content, 'html.parser')\n",
    "\n",
    "jobs = search_soup.select('.search-list .wht-shd-bx')\n",
    "job_data = []\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('h3').text.strip()\n",
    "    location = job.find('span', {'class': 'loc'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'company'}).text.strip()\n",
    "    experience_required = job.find('span', {'class': 'exp'}).text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': title,\n",
    "        'Job Location': location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05c6665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "form_data = {}\n",
    "for input_tag in soup.select('form input'):\n",
    "    form_data[input_tag.get('name')] = input_tag.get('value', '')\n",
    "\n",
    "form_data['search-keyword'] = job_title\n",
    "form_data['location'] = location\n",
    "\n",
    "search_response = requests.post(url, data=form_data)\n",
    "\n",
    "search_soup = BeautifulSoup(search_response.content, 'html.parser')\n",
    "\n",
    "jobs = search_soup.select('.search-list .wht-shd-bx')\n",
    "job_data = []\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('h3').text.strip()\n",
    "    location = job.find('span', {'class': 'loc'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'company'}).text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': title,\n",
    "        'Job Location': location,\n",
    "        'Company Name': company_name\n",
    "    })\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc83f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_title = \"Data Scientist\"\n",
    "\n",
    "form_data = {}\n",
    "for input_tag in soup.select('form input'):\n",
    "    form_data[input_tag.get('name')] = input_tag.get('value', '')\n",
    "\n",
    "form_data['search-keyword'] = job_title\n",
    "\n",
    "search_response = requests.post(url, data=form_data)\n",
    "\n",
    "search_soup = BeautifulSoup(search_response.content, 'html.parser')\n",
    "\n",
    "location_filter = \"Delhi/NCR\"\n",
    "salary_filter = \"3-6 Lakhs\"\n",
    "\n",
    "filter_data = {}\n",
    "for input_tag in search_soup.select('#filterForm input'):\n",
    "    filter_data[input_tag.get('name')] = input_tag.get('value', '')\n",
    "\n",
    "filter_data['filter_location'] = location_filter\n",
    "filter_data['filter_salary'] = salary_filter\n",
    "\n",
    "\n",
    "filter_response = requests.post(url, data=filter_data)\n",
    "\n",
    "\n",
    "filtered_soup = BeautifulSoup(filter_response.content, 'html.parser')\n",
    "\n",
    "jobs = filtered_soup.select('.search-list .wht-shd-bx')\n",
    "job_data = []\n",
    "\n",
    "for job in jobs[:10]:\n",
    "    title = job.find('h3').text.strip()\n",
    "    location = job.find('span', {'class': 'loc'}).text.strip()\n",
    "    company_name = job.find('span', {'class': 'company'}).text.strip()\n",
    "    experience_required = job.find('span', {'class': 'exp'}).text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': title,\n",
    "        'Job Location': location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492e0c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "search_query = \"sunglasses\"\n",
    "total_sunglasses = 100\n",
    "products_per_page = 40\n",
    "pages_required = (total_sunglasses + products_per_page - 1) // products_per_page\n",
    "\n",
    "def scrape_sunglasses_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    sunglasses_data = []\n",
    "\n",
    "    for product in soup.find_all('div', {'class': '_1AtVbE'}):\n",
    "        brand = product.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "        description = product.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "        price = product.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "        sunglasses_data.append({\n",
    "            'Brand': brand,\n",
    "            'Product Description': description,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return sunglasses_data\n",
    "\n",
    "all_sunglasses_data = []\n",
    "\n",
    "for page in range(1, pages_required + 1):\n",
    "    search_url = f\"{base_url}/search?q={search_query}&page={page}\"\n",
    "    sunglasses_data = scrape_sunglasses_data(search_url)\n",
    "    all_sunglasses_data.extend(sunglasses_data)\n",
    "df = pd.DataFrame(all_sunglasses_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6867df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5    Worth every penny   \n",
      "1       5  Best in the market!   \n",
      "2       5            Wonderful   \n",
      "3       5       Classy product   \n",
      "4       5             Terrific   \n",
      "..    ...                  ...   \n",
      "95      5   Highly recommended   \n",
      "96      5            Wonderful   \n",
      "97      4            Very Good   \n",
      "98      5            Brilliant   \n",
      "99      5       Classy product   \n",
      "\n",
      "                                          Full Review  \n",
      "0   Feeling awesome after getting the delivery of ...  \n",
      "1                                Good CameraREAD MORE  \n",
      "2                     This is amazing at allREAD MORE  \n",
      "3   Camera is awesomeBest battery backupA performe...  \n",
      "4                             Very very goodREAD MORE  \n",
      "..                                                ...  \n",
      "95  Thanks Flipkart For this amazing deal! I had a...  \n",
      "96  Excellent Fabulous Adorable Iphone 11 Value fo...  \n",
      "97  I switched to IOS for long term use and for be...  \n",
      "98  Perfect iPhone on this budget!! Camera and the...  \n",
      "99        Outstanding performance this phoneREAD MORE  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "def scrape_reviews_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    reviews_data = []\n",
    "\n",
    "    for review in soup.find_all('div', {'class': '_27M-vq'}):\n",
    "        rating = review.find('div', {'class': '_3LWZlK'}).text.strip()\n",
    "        review_summary = review.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "        full_review = review.find('div', {'class': 't-ZTKy'}).text.strip()\n",
    "\n",
    "        reviews_data.append({\n",
    "            'Rating': rating,\n",
    "            'Review Summary': review_summary,\n",
    "            'Full Review': full_review\n",
    "        })\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "all_reviews_data = []\n",
    "\n",
    "current_page = 1\n",
    "while len(all_reviews_data) < 100:\n",
    "    reviews_url = f\"{url}&page={current_page}\"\n",
    "    reviews_data = scrape_reviews_data(reviews_url)\n",
    "    all_reviews_data.extend(reviews_data)\n",
    "    current_page += 1\n",
    "\n",
    "\n",
    "    if not reviews_data:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(all_reviews_data[:100])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef48d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_sneakers_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    sneakers_data = []\n",
    "\n",
    "    for product in soup.find_all('div', {'class': '_2WkVRV'}):\n",
    "        brand = product.find('div', {'class': '_2B_pmu'}).text.strip()\n",
    "        description = product.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "        price = product.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "        sneakers_data.append({\n",
    "            'Brand': brand,\n",
    "            'Product Description': description,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return sneakers_data\n",
    "\n",
    "all_sneakers_data = []\n",
    "base_url = \"https://www.flipkart.com/\"\n",
    "search_query = \"sneakers\"\n",
    "\n",
    "current_page = 1\n",
    "while len(all_sneakers_data) < 100:\n",
    "    search_url = f\"{base_url}/search?q={search_query}&page={current_page}\"\n",
    "    sneakers_data = scrape_sneakers_data(search_url)\n",
    "    all_sneakers_data.extend(sneakers_data)\n",
    "    current_page += 1\n",
    "\n",
    "    if not sneakers_data:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(all_sneakers_data[:100])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f335d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Core i7 filter not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_laptop_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptops_data = []\n",
    "\n",
    "    for product in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
    "        title = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
    "        ratings = product.find('span', {'class': 'a-icon-alt'})\n",
    "        ratings = ratings.text.strip() if ratings else 'N/A'\n",
    "        price = product.find('span', {'class': 'a-offscreen'}).text.strip()\n",
    "\n",
    "        laptops_data.append({\n",
    "            'Title': title,\n",
    "            'Ratings': ratings,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return laptops_data\n",
    "\n",
    "base_url = \"https://www.amazon.in/\"\n",
    "search_query = \"Laptop\"\n",
    "cpu_type_filter = \"Intel Core i7\"\n",
    "\n",
    "search_url = f\"{base_url}s?k={search_query}\"\n",
    "response = requests.get(search_url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "cpu_type_filter_url = \"\"\n",
    "for link in soup.find_all('a', {'class': 'a-link-normal', 'href': True}):\n",
    "    if \"Intel+Core+i7\" in link['href']:\n",
    "        cpu_type_filter_url = link['href']\n",
    "        break\n",
    "\n",
    "if cpu_type_filter_url:\n",
    "    filter_url = f\"{base_url}{cpu_type_filter_url}\"\n",
    "    laptops_data = scrape_laptop_data(filter_url)\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(laptops_data[:10])\n",
    "\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Intel Core i7 filter not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffe345",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6040202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_quotes_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    quotes_data = []\n",
    "\n",
    "    for quote in soup.find_all('div', {'class': 'wrap-block'}):\n",
    "        quote_text = quote.find('a', {'class': 'title'}).text.strip()\n",
    "        author = quote.find('a', {'class': 'author'}).text.strip()\n",
    "        quote_type = quote.find('div', {'class': 'kw-box'}).text.strip()\n",
    "\n",
    "        quotes_data.append({\n",
    "            'Quote': quote_text,\n",
    "            'Author': author,\n",
    "            'Type of Quote': quote_type\n",
    "        })\n",
    "\n",
    "    return quotes_data\n",
    "\n",
    "base_url = \"https://www.azquotes.com/\"\n",
    "top_quotes_url = f\"{base_url}top-quotes\"\n",
    "response = requests.get(top_quotes_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    top_quotes_data = scrape_quotes_data(top_quotes_url)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(top_quotes_data)\n",
    "\n",
    "\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd68bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce88d062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_prime_ministers_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    prime_ministers_data = []\n",
    "\n",
    "\n",
    "    return prime_ministers_data\n",
    "\n",
    "base_url = \"https://www.jagranjosh.com/\"\n",
    "gk_url = f\"{base_url}gk\"\n",
    "prime_ministers_url = f\"{gk_url}/list-of-all-prime-ministers-of-india\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    prime_ministers_data = scrape_prime_ministers_data(prime_ministers_url)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(prime_ministers_data)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de71dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97746e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_expensive_cars_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    expensive_cars_data = []\n",
    "\n",
    "    return expensive_cars_data\n",
    "\n",
    "base_url = \"https://www.motor1.com/\"\n",
    "search_query = \"50 most expensive cars\"\n",
    "search_url = f\"{base_url}search/?q={search_query}\"\n",
    "\n",
    "response = requests.get(search_url)\n",
    "if response.status_code == 200:\n",
    "    # Write code to click on the desired search result (you may need to inspect the HTML and find the appropriate link)\n",
    "\n",
    "\n",
    "    expensive_cars_data = scrape_expensive_cars_data(result_url)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(expensive_cars_data)\n",
    "\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3deaab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
