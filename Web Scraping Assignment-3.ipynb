{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 1 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a87d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_amazon\u001b[39m(product_name):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Set the path to your webdriver. Download the webdriver for your browser.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    # Set the path to your webdriver. Download the webdriver for your browser.\n",
    "    # For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "    # Navigate to Amazon.in\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "    # Find the search input field and enter the product name\n",
    "    search_box = driver.find_element(\"id\", \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Print the titles of the search results\n",
    "    results = driver.find_elements_by_css_selector('.s-title-instructions h2 a')\n",
    "    for index, result in enumerate(results, start=1):\n",
    "        print(f\"{index}. {result.text}\\n{result.get_attribute('href')}\\n\")\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Take user input for the product to be searched\n",
    "    product_to_search = input(\"Enter the product to be searched on Amazon: \")\n",
    "\n",
    "    # Perform the search\n",
    "    search_amazon(product_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d81574",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 2 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b7b87c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_amazon\u001b[39m(product_name):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Set the path to your webdriver. Download the webdriver for your browser.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_amazon(product_name):\n",
    "    # Set the path to your webdriver. Download the webdriver for your browser.\n",
    "    # For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "    # Navigate to Amazon.in\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "    # Find the search input field and enter the product name\n",
    "    search_box = driver.find_element(\"id\", \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Initialize data lists\n",
    "    brand_names = []\n",
    "    product_names = []\n",
    "    prices = []\n",
    "    return_exchange = []\n",
    "    expected_delivery = []\n",
    "    availability = []\n",
    "    product_urls = []\n",
    "\n",
    "    # Loop through the first 3 pages of search results\n",
    "    for page in range(3):\n",
    "        # Print the titles of the search results on the current page\n",
    "        results = driver.find_elements_by_css_selector('.s-title-instructions h2 a')\n",
    "        for result in results:\n",
    "            # Click on each product to get detailed information\n",
    "            result.click()\n",
    "\n",
    "            # Wait for the product details page to load\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            # Scrape details from the product page\n",
    "            brand_name = driver.find_element_by_id(\"bylineInfo\").text.strip() if driver.find_element_by_id(\"bylineInfo\").text.strip() else \"-\"\n",
    "            product_name = driver.find_element_by_id(\"productTitle\").text.strip() if driver.find_element_by_id(\"productTitle\").text.strip() else \"-\"\n",
    "            price = driver.find_element_by_id(\"priceblock_ourprice\").text.strip() if driver.find_element_by_id(\"priceblock_ourprice\").text.strip() else \"-\"\n",
    "            return_exchange_info = driver.find_element_by_xpath(\"//span[contains(text(),'Return policy')]/following-sibling::span\").text.strip() if driver.find_element_by_xpath(\"//span[contains(text(),'Return policy')]/following-sibling::span\").text.strip() else \"-\"\n",
    "            expected_delivery_info = driver.find_element_by_xpath(\"//div[@id='ddmDeliveryMessage']/b\").text.strip() if driver.find_element_by_xpath(\"//div[@id='ddmDeliveryMessage']/b\").text.strip() else \"-\"\n",
    "            availability_info = driver.find_element_by_id(\"availability\").text.strip() if driver.find_element_by_id(\"availability\").text.strip() else \"-\"\n",
    "            product_url = driver.current_url.strip() if driver.current_url.strip() else \"-\"\n",
    "\n",
    "            # Append details to the lists\n",
    "            brand_names.append(brand_name)\n",
    "            product_names.append(product_name)\n",
    "            prices.append(price)\n",
    "            return_exchange.append(return_exchange_info)\n",
    "            expected_delivery.append(expected_delivery_info)\n",
    "            availability.append(availability_info)\n",
    "            product_urls.append(product_url)\n",
    "\n",
    "            # Go back to the search results page\n",
    "            driver.back()\n",
    "\n",
    "        # Navigate to the next page of search results\n",
    "        next_page_button = driver.find_element_by_partial_link_text(\"Next\")\n",
    "        if next_page_button:\n",
    "            next_page_button.click()\n",
    "            driver.implicitly_wait(5)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame with the scraped data\n",
    "    data = {\n",
    "        \"Brand Name\": brand_names,\n",
    "        \"Name of the Product\": product_names,\n",
    "        \"Price\": prices,\n",
    "        \"Return/Exchange\": return_exchange,\n",
    "        \"Expected Delivery\": expected_delivery,\n",
    "        \"Availability\": availability,\n",
    "        \"Product URL\": product_urls,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f\"{product_name}_search_results.csv\", index=False)\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Take user input for the product to be searched\n",
    "    product_to_search = input(\"Enter the product to be searched on Amazon: \")\n",
    "\n",
    "    # Perform the search and scraping\n",
    "    scrape_amazon(product_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1292d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 3 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f46a162",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def save_images(keyword, image_urls):\n",
    "    # Create a directory for each keyword\n",
    "    directory = f\"{keyword}_images\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Download and save each image\n",
    "    for i, url in enumerate(image_urls):\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(os.path.join(directory, f\"{keyword}_{i+1}.jpg\"), 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "\n",
    "def scrape_images(keyword):\n",
    "    # Set the path to your webdriver. Download the webdriver for your browser.\n",
    "    # For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "    # Navigate to Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    # Find the search bar and enter the keyword\n",
    "    search_bar = driver.find_element(\"name\", \"q\")\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll down to load more images\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Get the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find image elements and extract URLs\n",
    "    image_elements = soup.find_all('img', class_='rg_i')\n",
    "    image_urls = [element['src'] for element in image_elements]\n",
    "\n",
    "    # Save 10 images for the specified keyword\n",
    "    save_images(keyword, image_urls[:10])\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of keywords to search for\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for keyword: {keyword}\")\n",
    "        scrape_images(keyword)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 4 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b4ac59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_flipkart_smartphones(product_name):\n",
    "    # Set the path to your webdriver. Download the webdriver for your browser.\n",
    "    # For Chrome: https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "    # Navigate to Flipkart\n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "    # Find the search bar and enter the product name\n",
    "    search_bar = driver.find_element(\"name\", \"q\")\n",
    "    search_bar.send_keys(product_name)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results to load\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Get the page source after searching\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find smartphone elements on the first page\n",
    "    smartphone_elements = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "    # Initialize data lists\n",
    "    brand_names = []\n",
    "    smartphone_names = []\n",
    "    colors = []\n",
    "    rams = []\n",
    "    storages = []\n",
    "    primary_cameras = []\n",
    "    secondary_cameras = []\n",
    "    display_sizes = []\n",
    "    battery_capacities = []\n",
    "    prices = []\n",
    "    product_urls = []\n",
    "\n",
    "    # Extract details from each smartphone element\n",
    "    for element in smartphone_elements:\n",
    "        # Brand and smartphone name\n",
    "        brand_name = element.find('div', class_='_4rR01T').text.strip() if element.find('div', class_='_4rR01T') else \"-\"\n",
    "        smartphone_name = element.find('a', class_='IRpwTa').text.strip() if element.find('a', class_='IRpwTa') else \"-\"\n",
    "\n",
    "        # Other details\n",
    "        details = element.find_all('li', class_='rgWa7D')\n",
    "        color = details[0].text.strip() if details and len(details) > 0 else \"-\"\n",
    "        ram = details[1].text.strip() if details and len(details) > 1 else \"-\"\n",
    "        storage = details[2].text.strip() if details and len(details) > 2 else \"-\"\n",
    "        primary_camera = details[3].text.strip() if details and len(details) > 3 else \"-\"\n",
    "        secondary_camera = details[4].text.strip() if details and len(details) > 4 else \"-\"\n",
    "        display_size = details[5].text.strip() if details and len(details) > 5 else \"-\"\n",
    "        battery_capacity = details[6].text.strip() if details and len(details) > 6 else \"-\"\n",
    "        price = element.find('div', class_='_30jeq3').text.strip() if element.find('div', class_='_30jeq3') else \"-\"\n",
    "        product_url = \"https://www.flipkart.com\" + element.find('a', class_='IRpwTa')['href'] if element.find('a', class_='IRpwTa') else \"-\"\n",
    "\n",
    "        # Append details to the lists\n",
    "        brand_names.append(brand_name)\n",
    "        smartphone_names.append(smartphone_name)\n",
    "        colors.append(color)\n",
    "        rams.append(ram)\n",
    "        storages.append(storage)\n",
    "        primary_cameras.append(primary_camera)\n",
    "        secondary_cameras.append(secondary_camera)\n",
    "        display_sizes.append(display_size)\n",
    "        battery_capacities.append(battery_capacity)\n",
    "        prices.append(price)\n",
    "        product_urls.append(product_url)\n",
    "\n",
    "    # Create a DataFrame with the scraped data\n",
    "    data = {\n",
    "        \"Brand Name\": brand_names,\n",
    "        \"Smartphone Name\": smartphone_names,\n",
    "        \"Colour\": colors,\n",
    "        \"RAM\": rams,\n",
    "        \"Storage(ROM)\": storages,\n",
    "        \"Primary Camera\": primary_cameras,\n",
    "        \"Secondary Camera\": secondary_cameras,\n",
    "        \"Display Size\": display_sizes,\n",
    "        \"Battery Capacity\": battery_capacities,\n",
    "        \"Price\": prices,\n",
    "        \"Product URL\": product_urls,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f\"{product_name}_search_results.csv\", index=False)\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Take user input for the smartphone to be searched\n",
    "    product_to_search = input(\"Enter the smartphone to be searched on Flipkart: \")\n",
    "\n",
    "    # Perform the search and scraping\n",
    "    scrape_flipkart_smartphones(product_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7563c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34204568",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4124411766.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install googlemaps\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install googlemaps\n",
    "import googlemaps\n",
    "\n",
    "def get_coordinates(api_key, city_name):\n",
    "    # Create a Google Maps client\n",
    "    gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "    # Geocode the city\n",
    "    geocode_result = gmaps.geocode(city_name)\n",
    "\n",
    "    # Extract the latitude and longitude\n",
    "    if geocode_result and 'geometry' in geocode_result[0] and 'location' in geocode_result[0]['geometry']:\n",
    "        location = geocode_result[0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google Maps API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Take user input for the city to be searched\n",
    "    city_name = input(\"Enter the city name: \")\n",
    "\n",
    "    # Get the geospatial coordinates\n",
    "    coordinates = get_coordinates(api_key, city_name)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Geospatial Coordinates (Latitude, Longitude) for {city_name}: {coordinates}\")\n",
    "    else:\n",
    "        print(f\"Unable to retrieve geospatial coordinates for {city_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 6 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1907f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to 'gaming_laptops.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "\n",
    "        data = {\n",
    "            \"Name\": [],\n",
    "            \"Processor\": [],\n",
    "            \"RAM\": [],\n",
    "            \"OS\": [],\n",
    "            \"Storage\": [],\n",
    "            \"Display\": [],\n",
    "            \"Price\": []\n",
    "        }\n",
    "\n",
    "        for laptop in laptops:\n",
    "            name = laptop.find('div', class_='TopNumbeHeadingname').text.strip()\n",
    "            processor = laptop.find('div', class_='TopNumbeHeadingpname').text.strip()\n",
    "            ram = laptop.find('div', class_='TopNumbeHeadingnname').text.strip()\n",
    "            os = laptop.find('div', class_='TopNumbeHeadingname').text.strip()\n",
    "            storage = laptop.find('div', class_='TopNumbeHeadingname').text.strip()\n",
    "            display = laptop.find('div', class_='TopNumbeHeadingname').text.strip()\n",
    "            price = laptop.find('div', class_='TopNumbeHeadingprice').text.strip()\n",
    "\n",
    "            data[\"Name\"].append(name)\n",
    "            data[\"Processor\"].append(processor)\n",
    "            data[\"RAM\"].append(ram)\n",
    "            data[\"OS\"].append(os)\n",
    "            data[\"Storage\"].append(storage)\n",
    "            data[\"Display\"].append(display)\n",
    "            data[\"Price\"].append(price)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "        print(\"Data has been scraped and saved to 'gaming_laptops.csv'\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status Code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_digit_gaming_laptops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qustions 7 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae4abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to 'forbes_billionaires.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        billionaires = soup.find_all('div', class_='personListItem')\n",
    "\n",
    "        data = {\n",
    "            \"Rank\": [],\n",
    "            \"Name\": [],\n",
    "            \"Net Worth\": [],\n",
    "            \"Age\": [],\n",
    "            \"Citizenship\": [],\n",
    "            \"Source\": [],\n",
    "            \"Industry\": []\n",
    "        }\n",
    "\n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire.find('div', class_='rank').text.strip()\n",
    "            name = billionaire.find('div', class_='name').text.strip()\n",
    "            net_worth = billionaire.find('div', class_='netWorth').text.strip()\n",
    "            age = billionaire.find('div', class_='age').text.strip()\n",
    "            citizenship = billionaire.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = billionaire.find('div', class_='source').text.strip()\n",
    "            industry = billionaire.find('div', class_='category').text.strip()\n",
    "\n",
    "            data[\"Rank\"].append(rank)\n",
    "            data[\"Name\"].append(name)\n",
    "            data[\"Net Worth\"].append(net_worth)\n",
    "            data[\"Age\"].append(age)\n",
    "            data[\"Citizenship\"].append(citizenship)\n",
    "            data[\"Source\"].append(source)\n",
    "            data[\"Industry\"].append(industry)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "        print(\"Data has been scraped and saved to 'forbes_billionaires.csv'\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status Code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 8 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68519e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_youtube_comments\u001b[39m(api_key, video_id, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "def get_youtube_comments(api_key, video_id, max_results=500):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get video details\n",
    "    video_response = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    video_title = video_response['items'][0]['snippet']['title']\n",
    "\n",
    "    # Get comments\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        comment_response = youtube.commentThreads().list(\n",
    "            part='snippet,replies',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_results - len(comments), 100),\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in comment_response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'UpvoteCount': comment['likeCount'],\n",
    "                'Time': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = comment_response.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(comments)\n",
    "    df.to_csv(f'{video_title}_comments.csv', index=False)\n",
    "    print(f\"Data has been scraped and saved to '{video_title}_comments.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual YouTube API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Replace 'VIDEO_ID' with the actual video ID you want to scrape comments from\n",
    "    video_id = 'VIDEO_ID'\n",
    "\n",
    "    # Get YouTube comments\n",
    "    get_youtube_comments(api_key, video_id, max_results=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions 9 Ans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9336043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostelworld_london():\n",
    "    url = \"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2022-03-07&to=2022-03-14&guests=1&page=1\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        hostels = soup.find_all('div', class_='property-card')\n",
    "\n",
    "        data = {\n",
    "            \"Hostel Name\": [],\n",
    "            \"Distance from City Centre\": [],\n",
    "            \"Ratings\": [],\n",
    "            \"Total Reviews\": [],\n",
    "            \"Overall Reviews\": [],\n",
    "            \"Privates from Price\": [],\n",
    "            \"Dorms from Price\": [],\n",
    "            \"Facilities\": [],\n",
    "            \"Property Description\": []\n",
    "        }\n",
    "\n",
    "        for hostel in hostels:\n",
    "            name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance = hostel.find('span', class_='description').text.strip()\n",
    "            ratings = hostel.find('div', class_='score orange big').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip()\n",
    "            overall_reviews = hostel.find('div', class_='reviews').text.strip()\n",
    "            privates_price = hostel.find('div', class_='price-col privates-from').text.strip()\n",
    "            dorms_price = hostel.find('div', class_='price-col dorms-from').text.strip()\n",
    "            facilities = [facility.text.strip() for facility in hostel.find_all('div', class_='facilities-item')]\n",
    "            description = hostel.find('div', class_='property-card-text').text.strip()\n",
    "\n",
    "            data[\"Hostel Name\"].append(name)\n",
    "            data[\"Distance from City Centre\"].append(distance)\n",
    "            data[\"Ratings\"].append(ratings)\n",
    "            data[\"Total Reviews\"].append(total_reviews)\n",
    "            data[\"Overall Reviews\"].append(overall_reviews)\n",
    "            data[\"Privates from Price\"].append(privates_price)\n",
    "            data[\"Dorms from Price\"].append(dorms_price)\n",
    "            data[\"Facilities\"].append(', '.join(facilities))\n",
    "            data[\"Property Description\"].append(description)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"hostelworld_london.csv\", index=False)\n",
    "        print(\"Data has been scraped and saved to 'hostelworld_london.csv'\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status Code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hostelworld_london()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
